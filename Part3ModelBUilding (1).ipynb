{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a285f7a7-d0c0-4f0b-827f-efe58beb6b85",
   "metadata": {},
   "source": [
    "For predicting customer churn from the Telco dataset, three widely used machine learning models that typically perform well for binary classification tasks are:\n",
    "\n",
    "1. **Logistic Regression:**\n",
    "   - **Why Choose It:** Logistic Regression is a straightforward model that works well for binary classification problems. It's interpretable, fast to train, and provides probabilities for outcomes, which can be useful for setting thresholds for actions.\n",
    "   - **Suitability:** It performs well when the relationship between the independent variables and the log-odds of the dependent variable is linear, and it's a good baseline model to start with in binary classification.\n",
    "\n",
    "2. **Random Forest Classifier:**\n",
    "   - **Why Choose It:** Random Forest is a robust ensemble technique that uses multiple decision trees to make predictions, reducing the risk of overfitting. It can handle both numerical and categorical data, is robust to outliers, and can model non-linear relationships.\n",
    "   - **Suitability:** It's effective in high dimensional spaces as well as large data sets and provides feature importance scores, which help in understanding the most significant predictors of churn.\n",
    "\n",
    "3. **Gradient Boosting Classifier (e.g., XGBoost, LightGBM):**\n",
    "   - **Why Choose It:** Gradient Boosting Machines (GBMs) are powerful ensemble learning techniques that build trees one at a time, where each new tree helps to correct errors made by previously trained trees. They have been known to deliver high accuracy in many binary classification problems.\n",
    "   - **Suitability:** GBMs can handle missing data and, like Random Forest, provide feature importance. They are highly flexible and can be optimized to achieve better performance through hyperparameter tuning.\n",
    "\n",
    "### Considerations for Model Choice:\n",
    "- **Data Size and Quality:** Logistic Regression requires less computational resources, making it suitable for smaller or less complex datasets, while Random Forest and GBM are better for larger datasets.\n",
    "- **Interpretability:** Logistic Regression offers the best interpretability. If understanding the influence of each predictor is important, this might be preferable.\n",
    "- **Accuracy vs Speed:** GBMs often provide the best accuracy but at the cost of increased computational time and complexity. Random Forest strikes a balance between accuracy and training speed.\n",
    "\n",
    "Next steps would involve preparing the data for these models, selecting features based on importance and relevance to churn, and then tuning each model to compare their performance accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e539120-60b5-44be-a071-ed2141919148",
   "metadata": {},
   "source": [
    "Neural networks are another powerful option for predicting customer churn, especially when dealing with large datasets and complex relationships among features. Here’s why you might consider using a neural network for churn prediction:\n",
    "\n",
    "### Advantages of Using Neural Networks:\n",
    "1. **Complex Pattern Recognition:**\n",
    "   - Neural networks excel at identifying complex patterns and interactions between features that may not be easily captured by traditional machine learning models.\n",
    "  \n",
    "2. **Handling High-Dimensional Data:**\n",
    "   - They can manage large volumes of data with many features, making them suitable for datasets that include various customer behaviors and attributes.\n",
    "  \n",
    "3. **Flexibility and Customization:**\n",
    "   - The architecture of a neural network can be customized and tuned, including the number of hidden layers, the number of neurons in each layer, activation functions, etc., to optimize performance.\n",
    "\n",
    "4. **Scalability:**\n",
    "   - They scale well with additional data, often improving in accuracy as more training data becomes available.\n",
    "\n",
    "### Considerations When Using Neural Networks for Churn Prediction:\n",
    "1. **Data Requirements:**\n",
    "   - Neural networks generally require larger datasets to perform well without overfitting compared to simpler models. They may also require more preprocessing, such as normalization of all input features.\n",
    "\n",
    "2. **Complexity and Training Time:**\n",
    "   - They are typically more computationally intensive and take longer to train than most traditional machine learning models, especially as you scale up the number of layers and units.\n",
    "\n",
    "3. **Interpretability:**\n",
    "   - Unlike models like logistic regression or decision trees, neural networks are often considered \"black boxes\" because it can be difficult to interpret exactly how the model is making its decisions.\n",
    "\n",
    "4. **Overfitting Risk:**\n",
    "   - There is a higher risk of overfitting with neural networks, particularly if the network is too complex or if not enough training data is available. Techniques such as dropout, regularization, and proper validation can help mitigate this risk.\n",
    "\n",
    "### Typical Neural Network Setup for Churn Prediction:\n",
    "- **Input Layer:** Should have as many neurons as the number of features in the dataset.\n",
    "- **Hidden Layers:** Depending on the complexity, one or more hidden layers can be added. For churn prediction, starting with one or two layers is common.\n",
    "- **Activation Functions:** ReLU (Rectified Linear Activation) is a common choice for hidden layers due to its efficiency, while the output layer might use a sigmoid function for binary classification to output probabilities.\n",
    "- **Output Layer:** For binary classification (churn or not), the output layer should have a single neuron.\n",
    "- **Loss Function:** Binary cross-entropy is typically used for binary classification tasks.\n",
    "- **Optimizer:** Adam or SGD (Stochastic Gradient Descent) are commonly used to minimize the loss function.\n",
    "\n",
    "Incorporating a neural network into your churn prediction pipeline could potentially increase your model's performance, particularly if tuned correctly and provided with enough data. However, consider the trade-offs between performance, training time, and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e8b48-96a3-44f1-a216-9fb31f3c1b0f",
   "metadata": {},
   "source": [
    "# 1 daa prep just if u missed Part 1 main topic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94416409-6a39-492d-98a8-5831e9329501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not execute for reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369a7d6c-5b50-4be4-96bc-14ded737dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df= pd.read_csv('D03Mainfully_processed_telco_data.csv')\n",
    "# Drop non-numeric columns explicitly if any, e.g., 'customerID' if it's not encoded\n",
    "if 'customerID' in df.columns:\n",
    "    df = df.drop(columns=['customerID'])\n",
    "\n",
    "# Assuming 'Churn_Yes' is the target and already encoded as numeric\n",
    "X = df.drop('Churn_Yes', axis=1)\n",
    "y = df['Churn_Yes']\n",
    "\n",
    "# Identify numeric columns only for scaling\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling only to numeric columns\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e3324-a4ad-4195-a211-924ac445153f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603af581-33c9-4d34-9900-2fb77e227ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d109184-4059-4859-abc4-11b35e9db39e",
   "metadata": {},
   "source": [
    "# 2 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32cfab3e-da3a-4ecb-a193-9c0d3e8a4f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8069552874378992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.93      0.88      1036\n",
      "        True       0.70      0.48      0.57       373\n",
      "\n",
      "    accuracy                           0.81      1409\n",
      "   macro avg       0.76      0.70      0.72      1409\n",
      "weighted avg       0.80      0.81      0.79      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('D02fully_processed_telco_data.csv')\n",
    "\n",
    "# Assuming 'Churn_Yes' is the target\n",
    "X = df.drop('Churn_Yes', axis=1)\n",
    "y = df['Churn_Yes']\n",
    "\n",
    "# Identify numeric columns only for scaling\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Impute any missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "# Create and train Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "log_reg_predictions = log_reg.predict(X_test_imputed)\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_predictions)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_accuracy)\n",
    "print(classification_report(y_test, log_reg_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6966b-06cc-4a3a-aace-8c9edeeb87fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73a4c5a3-b309-44af-a0e2-f1ab89d1a417",
   "metadata": {},
   "source": [
    "# 3 Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f70c14-6f08-406c-92de-54aebe0a5d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7665010645848119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.88      0.85      1036\n",
      "        True       0.57      0.46      0.51       373\n",
      "\n",
      "    accuracy                           0.77      1409\n",
      "   macro avg       0.70      0.67      0.68      1409\n",
      "weighted avg       0.75      0.77      0.76      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('D02fully_processed_telco_data.csv')\n",
    "\n",
    "# Assuming 'Churn_Yes' is the target\n",
    "X = df.drop('Churn_Yes', axis=1)\n",
    "y = df['Churn_Yes']\n",
    "\n",
    "# Identify numeric columns for scaling\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Impute any missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "rf_predictions = rf_classifier.predict(X_test_imputed)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(classification_report(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bacf94-49f8-4abc-a183-9c84f94f46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91c7ca7-11b4-4aeb-9416-518a1333f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7693399574166075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.87      0.85      1036\n",
      "        True       0.58      0.48      0.52       373\n",
      "\n",
      "    accuracy                           0.77      1409\n",
      "   macro avg       0.70      0.68      0.69      1409\n",
      "weighted avg       0.76      0.77      0.76      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('D02fully_processed_telco_data.csv')\n",
    "\n",
    "# Assuming 'Churn_Yes' is the target\n",
    "X = df.drop('Churn_Yes', axis=1)\n",
    "y = df['Churn_Yes']\n",
    "\n",
    "# Identify numeric columns for scaling\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Impute any missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf_classifier.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "rf_predictions = rf_classifier.predict(X_test_imputed)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(classification_report(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066a1bf-adcd-44f3-bc3d-7632517b15f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acee28-8845-4942-8830-4fa1b1b5b1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c33c17e-6305-44ef-a6e7-6431898b11ae",
   "metadata": {},
   "source": [
    "# 4 XGBoost¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170a33a-f0f2-4874-ada3-3764729682fa",
   "metadata": {},
   "source": [
    "4)Using XGBoost for churn prediction is a great choice due to its performance in handling various types of data, its ability to manage missing values, and its effectiveness in binary classification tasks. Here’s how you can set up an XGBoost model, from data preparation to training and evaluation:\n",
    "\n",
    "XGBoost Model Setup XGBoost (eXtreme Gradient Boosting) is an implementation of gradient boosted decision trees designed for speed and performance. It is often used for its performance and flexibility in machine learning competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5596d2b3-9e79-4d24-b66b-f37d9b1a3ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in d:\\cprogramfiles\\anaconda2\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in d:\\cprogramfiles\\anaconda2\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\cprogramfiles\\anaconda2\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09784dc5-4d8e-4150-b7d7-265fdd00305b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8026969481902059\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.92      0.87      1036\n",
      "        True       0.69      0.47      0.56       373\n",
      "\n",
      "    accuracy                           0.80      1409\n",
      "   macro avg       0.76      0.70      0.72      1409\n",
      "weighted avg       0.79      0.80      0.79      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('D02fully_processed_telco_data.csv')\n",
    "\n",
    "# Assuming 'Churn_Yes' is the target\n",
    "X = df.drop('Churn_Yes', axis=1)\n",
    "y = df['Churn_Yes']\n",
    "\n",
    "# Identify numeric columns for scaling\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Impute any missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "# Convert data to DMatrix object, which is optimized for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_imputed, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_imputed, label=y_test)\n",
    "\n",
    "# Define XGBoost model parameters\n",
    "params = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.1,      # the training step for each iteration\n",
    "    'objective': 'binary:logistic',  # binary classification\n",
    "    'eval_metric': 'logloss',  # evaluation metric\n",
    "    'seed': 42       # for reproducible results\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_proba = bst.predict(dtest)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"XGBoost Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74711a1-4d82-425c-9c0a-94ccfc676605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increainsg the dept by 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9119fe29-ede0-4325-9b72-7bacf5a9aa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8041163946061036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.92      0.87      1036\n",
      "        True       0.69      0.48      0.56       373\n",
      "\n",
      "    accuracy                           0.80      1409\n",
      "   macro avg       0.76      0.70      0.72      1409\n",
      "weighted avg       0.79      0.80      0.79      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('D02fully_processed_telco_data.csv')\n",
    "\n",
    "# Assuming 'Churn_Yes' is the target\n",
    "X = df.drop('Churn_Yes', axis=1)\n",
    "y = df['Churn_Yes']\n",
    "\n",
    "# Identify numeric columns for scaling\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Impute any missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "# Convert data to DMatrix object, which is optimized for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_imputed, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_imputed, label=y_test)\n",
    "\n",
    "# Define XGBoost model parameters\n",
    "params = {\n",
    "    'max_depth': 5,  # the maximum depth of each tree\n",
    "    'eta': 0.1,      # the training step for each iteration\n",
    "    'objective': 'binary:logistic',  # binary classification\n",
    "    'eval_metric': 'logloss',  # evaluation metric\n",
    "    'seed': 42       # for reproducible results\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_proba = bst.predict(dtest)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"XGBoost Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999d54d-c8c9-43c3-abee-1ab81470a2f1",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Data Handling: XGBoost can handle missing values internally, so technically, you don't need to impute them (but you can if consistency with other models is necessary).\n",
    "\n",
    "DMatrix: XGBoost uses a DMatrix, an internal data structure optimized for both memory efficiency and training speed.\n",
    "\n",
    "Model Parameters:\n",
    "max_depth controls the depth of the trees. Deeper trees can model more complex patterns but can lead to overfitting.\n",
    "eta is the learning rate. Smaller values make the boosting process more conservative.\n",
    "objective specifies the learning task and the corresponding learning objective. For binary classification, it is set to binary:logistic.\n",
    "eval_metric is used to evaluate the training performance, logloss is typical for classification.\n",
    "Training and Prediction: Training is done using xgb.train, and predictions are made where probabilities greater than 0.5 are considered class 1 (churn).\n",
    "\n",
    "Evaluation: The model's accuracy and other classification metrics are calculated and printed.\n",
    "Additional Considerations:\n",
    "Hyperparameter Tuning: XGBoost performance can significantly improve by tuning hyperparameters like max_depth, min_child_weight, subsample, and colsample_bytree. Consider using tools like GridSearchCV or RandomizedSearchCV for this purpose.\n",
    "\n",
    "Cross-validation: XGBoost supports k-fold cross-validation via the xgb.cv method, which can be useful for more robust model evaluation.\n",
    "This setup provides a strong starting point for using XGBoost in your churn prediction task, with flexibility to adapt the model as needed based on your specific dataset characteristics and business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59d51d-c987-4bb0-9fe0-25ac13b8b6fa",
   "metadata": {},
   "source": [
    "# if by chance more rows were there , than XG boost would be best than Logistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0343ea7-5102-4088-9f9b-b3682ad7e015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced4b88-0b6b-4bd2-ba08-9f967b4129dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7074fbbb-ad1e-4fd6-bae8-b3dca76847fd",
   "metadata": {},
   "source": [
    "# 5 models - neural nets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f06df0-3d4d-442a-b5f1-4ccc492de185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow-intel==2.17.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading grpcio-1.66.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\soura\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading optree-0.12.1-cp311-cp311-win_amd64.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.7/48.7 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.17.0-cp311-cp311-win_amd64.whl (2.0 kB)\n",
      "Downloading tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl (385.0 MB)\n",
      "   ---------------------------------------- 0.0/385.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/385.0 MB 2.4 MB/s eta 0:02:41\n",
      "   ---------------------------------------- 0.3/385.0 MB 3.5 MB/s eta 0:01:49\n",
      "   ---------------------------------------- 0.7/385.0 MB 4.6 MB/s eta 0:01:24\n",
      "   ---------------------------------------- 1.1/385.0 MB 6.4 MB/s eta 0:01:01\n",
      "   ---------------------------------------- 1.6/385.0 MB 7.1 MB/s eta 0:00:55\n",
      "   ---------------------------------------- 2.1/385.0 MB 7.5 MB/s eta 0:00:51\n",
      "   ---------------------------------------- 2.7/385.0 MB 8.1 MB/s eta 0:00:48\n",
      "   ---------------------------------------- 3.2/385.0 MB 8.8 MB/s eta 0:00:44\n",
      "   ---------------------------------------- 3.7/385.0 MB 9.1 MB/s eta 0:00:42\n",
      "   ---------------------------------------- 4.3/385.0 MB 9.4 MB/s eta 0:00:41\n",
      "    --------------------------------------- 4.8/385.0 MB 9.7 MB/s eta 0:00:40\n",
      "    --------------------------------------- 5.4/385.0 MB 9.8 MB/s eta 0:00:39\n",
      "    --------------------------------------- 6.0/385.0 MB 10.1 MB/s eta 0:00:38\n",
      "    --------------------------------------- 6.5/385.0 MB 10.0 MB/s eta 0:00:39\n",
      "    --------------------------------------- 7.0/385.0 MB 10.2 MB/s eta 0:00:38\n",
      "    --------------------------------------- 7.7/385.0 MB 10.4 MB/s eta 0:00:37\n",
      "    --------------------------------------- 8.2/385.0 MB 10.4 MB/s eta 0:00:37\n",
      "    --------------------------------------- 8.8/385.0 MB 10.4 MB/s eta 0:00:37\n",
      "    --------------------------------------- 9.3/385.0 MB 10.7 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 9.9/385.0 MB 10.6 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 10.5/385.0 MB 11.5 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 11.0/385.0 MB 11.9 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 11.6/385.0 MB 11.9 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 12.1/385.0 MB 11.7 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 12.8/385.0 MB 11.9 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 13.2/385.0 MB 11.9 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 13.8/385.0 MB 11.9 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 14.2/385.0 MB 11.9 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 14.2/385.0 MB 11.9 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 14.2/385.0 MB 11.9 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 14.4/385.0 MB 10.6 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 14.8/385.0 MB 10.2 MB/s eta 0:00:37\n",
      "   - -------------------------------------- 15.0/385.0 MB 9.9 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 15.3/385.0 MB 9.6 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 15.7/385.0 MB 9.5 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 16.0/385.0 MB 9.4 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 16.3/385.0 MB 9.2 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 16.7/385.0 MB 9.0 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 17.2/385.0 MB 9.1 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 17.8/385.0 MB 9.1 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 17.8/385.0 MB 9.1 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 17.8/385.0 MB 8.3 MB/s eta 0:00:45\n",
      "   -- ------------------------------------- 19.5/385.0 MB 9.0 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 20.0/385.0 MB 9.0 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 20.5/385.0 MB 9.0 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 21.1/385.0 MB 8.8 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 21.5/385.0 MB 9.0 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 22.1/385.0 MB 8.8 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 22.7/385.0 MB 9.0 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 23.2/385.0 MB 8.8 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 23.7/385.0 MB 9.1 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 24.3/385.0 MB 8.8 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 24.8/385.0 MB 10.1 MB/s eta 0:00:36\n",
      "   -- ------------------------------------- 25.4/385.0 MB 10.7 MB/s eta 0:00:34\n",
      "   -- ------------------------------------- 26.0/385.0 MB 10.9 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 26.5/385.0 MB 11.5 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 27.1/385.0 MB 11.7 MB/s eta 0:00:31\n",
      "   -- ------------------------------------- 27.6/385.0 MB 11.7 MB/s eta 0:00:31\n",
      "   -- ------------------------------------- 28.2/385.0 MB 13.1 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 28.8/385.0 MB 12.6 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 29.3/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 29.9/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 30.5/385.0 MB 11.7 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 31.0/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 31.6/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 32.2/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 32.7/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 33.2/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 33.8/385.0 MB 11.7 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 34.2/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 34.8/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 35.2/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 35.7/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 36.3/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 36.8/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 37.4/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 38.0/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 38.7/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 39.2/385.0 MB 11.9 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 39.6/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 40.2/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 40.8/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 41.2/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 41.8/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 42.3/385.0 MB 11.5 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 42.8/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 43.3/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 43.8/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 44.2/385.0 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 44.7/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 45.3/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 45.8/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 46.2/385.0 MB 11.7 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 46.7/385.0 MB 11.7 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 47.3/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 47.8/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 48.3/385.0 MB 11.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 48.9/385.0 MB 11.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 49.3/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 49.8/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 50.5/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 51.0/385.0 MB 11.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 51.5/385.0 MB 11.9 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 52.1/385.0 MB 11.9 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 52.6/385.0 MB 11.9 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 53.2/385.0 MB 11.9 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 53.7/385.0 MB 11.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 54.1/385.0 MB 11.9 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 54.6/385.0 MB 11.9 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 55.1/385.0 MB 11.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 55.6/385.0 MB 11.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 55.8/385.0 MB 11.3 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 56.2/385.0 MB 11.1 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 56.8/385.0 MB 11.3 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 57.3/385.0 MB 11.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 57.8/385.0 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 58.4/385.0 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 59.0/385.0 MB 11.5 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 59.6/385.0 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 60.1/385.0 MB 11.5 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 60.7/385.0 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 61.2/385.0 MB 11.5 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 61.8/385.0 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 62.4/385.0 MB 11.1 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 63.0/385.0 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 63.5/385.0 MB 11.5 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 64.0/385.0 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 64.6/385.0 MB 11.5 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 65.1/385.0 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 65.8/385.0 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 66.3/385.0 MB 11.7 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 66.9/385.0 MB 11.9 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 66.9/385.0 MB 11.9 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 66.9/385.0 MB 11.9 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 68.5/385.0 MB 11.9 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 69.1/385.0 MB 11.7 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 69.6/385.0 MB 11.9 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 70.1/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 70.7/385.0 MB 11.9 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 71.2/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 71.8/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 72.3/385.0 MB 11.9 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 72.8/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 73.3/385.0 MB 11.9 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 73.9/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 74.4/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 74.9/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 75.5/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 76.0/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 76.6/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 77.1/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 77.6/385.0 MB 12.6 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 78.2/385.0 MB 11.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 78.7/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 79.2/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 79.7/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 80.2/385.0 MB 11.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 80.7/385.0 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 81.3/385.0 MB 11.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 81.8/385.0 MB 11.5 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 82.4/385.0 MB 11.7 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 82.9/385.0 MB 11.7 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 83.5/385.0 MB 11.7 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 84.0/385.0 MB 11.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 84.5/385.0 MB 11.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 85.0/385.0 MB 11.7 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 85.6/385.0 MB 11.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 86.1/385.0 MB 11.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 86.5/385.0 MB 11.9 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 87.0/385.0 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 87.5/385.0 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 88.0/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 88.5/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 89.1/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 89.7/385.0 MB 12.1 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 90.2/385.0 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 90.8/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 91.3/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 91.8/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 92.4/385.0 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 92.8/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 93.4/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 94.0/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 94.4/385.0 MB 11.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 94.9/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 95.5/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 95.9/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 96.5/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 97.0/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 97.5/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 98.1/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 98.6/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 99.2/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 99.8/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 100.2/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 100.7/385.0 MB 11.9 MB/s eta 0:00:24\n",
      "   ---------- ---------------------------- 101.1/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 101.5/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 101.5/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 101.5/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 101.5/385.0 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 103.0/385.0 MB 10.9 MB/s eta 0:00:26\n",
      "   ---------- ---------------------------- 103.4/385.0 MB 10.7 MB/s eta 0:00:27\n",
      "   ---------- ---------------------------- 103.9/385.0 MB 10.7 MB/s eta 0:00:27\n",
      "   ---------- ---------------------------- 104.5/385.0 MB 10.7 MB/s eta 0:00:27\n",
      "   ---------- ---------------------------- 104.6/385.0 MB 10.6 MB/s eta 0:00:27\n",
      "   ---------- ---------------------------- 104.9/385.0 MB 10.2 MB/s eta 0:00:28\n",
      "   ---------- ---------------------------- 105.3/385.0 MB 10.2 MB/s eta 0:00:28\n",
      "   ---------- ---------------------------- 105.9/385.0 MB 10.2 MB/s eta 0:00:28\n",
      "   ---------- ---------------------------- 106.4/385.0 MB 10.4 MB/s eta 0:00:27\n",
      "   ---------- ---------------------------- 106.9/385.0 MB 10.4 MB/s eta 0:00:27\n",
      "   ---------- ---------------------------- 107.3/385.0 MB 10.2 MB/s eta 0:00:28\n",
      "   ---------- ---------------------------- 107.8/385.0 MB 10.4 MB/s eta 0:00:27\n",
      "   ---------- ---------------------------- 108.3/385.0 MB 10.2 MB/s eta 0:00:28\n",
      "   ----------- --------------------------- 108.8/385.0 MB 10.1 MB/s eta 0:00:28\n",
      "   ----------- --------------------------- 109.2/385.0 MB 10.2 MB/s eta 0:00:27\n",
      "   ----------- --------------------------- 109.7/385.0 MB 10.2 MB/s eta 0:00:27\n",
      "   ----------- --------------------------- 110.3/385.0 MB 10.4 MB/s eta 0:00:27\n",
      "   ----------- --------------------------- 110.9/385.0 MB 10.2 MB/s eta 0:00:27\n",
      "   ----------- --------------------------- 111.4/385.0 MB 10.6 MB/s eta 0:00:26\n",
      "   ----------- --------------------------- 111.9/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 112.4/385.0 MB 11.5 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 113.0/385.0 MB 11.3 MB/s eta 0:00:25\n",
      "   ----------- --------------------------- 113.6/385.0 MB 11.3 MB/s eta 0:00:25\n",
      "   ----------- --------------------------- 114.1/385.0 MB 11.3 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 114.7/385.0 MB 11.3 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 115.2/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 115.7/385.0 MB 11.7 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 116.2/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 116.7/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 117.2/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 117.7/385.0 MB 11.7 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 118.2/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 118.7/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 119.2/385.0 MB 11.7 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 119.7/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 120.2/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 120.7/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 121.3/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 121.9/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 122.5/385.0 MB 11.9 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 123.1/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 123.6/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 124.3/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 124.8/385.0 MB 11.7 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 125.3/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 125.9/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 126.3/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 126.8/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 127.4/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 128.0/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 128.5/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 128.9/385.0 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 129.4/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 129.9/385.0 MB 11.9 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 130.3/385.0 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 130.9/385.0 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 131.4/385.0 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 132.0/385.0 MB 11.5 MB/s eta 0:00:23\n",
      "   ------------- ------------------------- 132.5/385.0 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 133.0/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 133.6/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 134.2/385.0 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 134.7/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 135.3/385.0 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 135.8/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 136.4/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 136.9/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 137.5/385.0 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 138.1/385.0 MB 11.3 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 138.6/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 139.1/385.0 MB 11.7 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 139.7/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 140.3/385.0 MB 11.9 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 140.8/385.0 MB 11.9 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 141.2/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 141.8/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 142.3/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 142.9/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 143.5/385.0 MB 11.5 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 144.0/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 144.6/385.0 MB 11.5 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 145.1/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 145.7/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 146.3/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 146.8/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 147.4/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 147.9/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 148.5/385.0 MB 11.5 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 149.0/385.0 MB 11.5 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 149.5/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 150.0/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 150.6/385.0 MB 11.5 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 151.2/385.0 MB 11.7 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 151.7/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 152.2/385.0 MB 11.9 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 152.7/385.0 MB 11.9 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 153.2/385.0 MB 11.9 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 153.7/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 154.1/385.0 MB 11.9 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 154.6/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 155.2/385.0 MB 11.9 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 155.7/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 156.2/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 156.8/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 157.2/385.0 MB 11.9 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 157.7/385.0 MB 11.9 MB/s eta 0:00:20\n",
      "   ---------------- ---------------------- 158.3/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   ---------------- ---------------------- 158.7/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   ---------------- ---------------------- 159.2/385.0 MB 11.9 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 159.7/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   ---------------- ---------------------- 160.3/385.0 MB 11.9 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 160.9/385.0 MB 11.9 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 161.4/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   ---------------- ---------------------- 161.9/385.0 MB 11.7 MB/s eta 0:00:20\n",
      "   ---------------- ---------------------- 162.4/385.0 MB 11.9 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 162.9/385.0 MB 11.9 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 163.3/385.0 MB 11.9 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 163.8/385.0 MB 11.9 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 164.4/385.0 MB 11.9 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 164.9/385.0 MB 11.7 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 165.3/385.0 MB 11.9 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 165.6/385.0 MB 11.7 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 166.2/385.0 MB 11.7 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 166.7/385.0 MB 11.5 MB/s eta 0:00:20\n",
      "   ---------------- ---------------------- 167.2/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 167.8/385.0 MB 11.7 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 168.3/385.0 MB 11.7 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 168.8/385.0 MB 11.7 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 169.3/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 169.8/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 170.3/385.0 MB 11.7 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 170.8/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 171.3/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 171.8/385.0 MB 11.7 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 172.3/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 172.8/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 173.4/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 173.9/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 174.4/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 175.0/385.0 MB 11.5 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 175.6/385.0 MB 11.9 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 176.1/385.0 MB 11.7 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 176.4/385.0 MB 11.7 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 177.0/385.0 MB 11.9 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 177.5/385.0 MB 11.7 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 178.0/385.0 MB 11.7 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 178.6/385.0 MB 11.7 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 179.1/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 179.4/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 179.9/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 180.4/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 180.9/385.0 MB 11.3 MB/s eta 0:00:19\n",
      "   ------------------ -------------------- 181.5/385.0 MB 11.3 MB/s eta 0:00:19\n",
      "   ------------------ -------------------- 182.0/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 182.4/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 182.9/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 183.4/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 183.9/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 184.4/385.0 MB 11.3 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 184.9/385.0 MB 11.7 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 185.5/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 186.0/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 186.5/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 187.0/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 187.3/385.0 MB 11.3 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 187.9/385.0 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 188.3/385.0 MB 11.3 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 188.5/385.0 MB 11.3 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 188.7/385.0 MB 10.7 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 189.2/385.0 MB 10.6 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 189.7/385.0 MB 10.7 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 190.2/385.0 MB 10.7 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 190.8/385.0 MB 10.7 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 191.4/385.0 MB 10.7 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 191.8/385.0 MB 10.7 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 192.3/385.0 MB 10.6 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 192.9/385.0 MB 10.6 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 193.3/385.0 MB 10.7 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 193.8/385.0 MB 10.6 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 194.2/385.0 MB 10.6 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 194.7/385.0 MB 10.6 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 195.3/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 195.9/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 196.4/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 196.8/385.0 MB 10.4 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 197.3/385.0 MB 10.4 MB/s eta 0:00:19\n",
      "   -------------------- ------------------ 197.7/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 198.2/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 198.8/385.0 MB 10.9 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 199.2/385.0 MB 11.1 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 199.5/385.0 MB 11.3 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 199.8/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 200.4/385.0 MB 10.7 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 200.5/385.0 MB 10.2 MB/s eta 0:00:19\n",
      "   -------------------- ------------------ 201.0/385.0 MB 10.2 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 201.7/385.0 MB 10.4 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 202.1/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 202.6/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 203.3/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 203.7/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 204.3/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 204.9/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 205.4/385.0 MB 10.6 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 205.9/385.0 MB 10.6 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 206.4/385.0 MB 10.6 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 206.8/385.0 MB 10.6 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 207.2/385.0 MB 10.4 MB/s eta 0:00:18\n",
      "   --------------------- ----------------- 207.7/385.0 MB 10.2 MB/s eta 0:00:18\n",
      "   --------------------- ----------------- 208.2/385.0 MB 10.4 MB/s eta 0:00:18\n",
      "   --------------------- ----------------- 208.7/385.0 MB 10.7 MB/s eta 0:00:17\n",
      "   --------------------- ----------------- 209.2/385.0 MB 10.9 MB/s eta 0:00:17\n",
      "   --------------------- ----------------- 209.6/385.0 MB 10.7 MB/s eta 0:00:17\n",
      "   --------------------- ----------------- 210.1/385.0 MB 11.3 MB/s eta 0:00:16\n",
      "   --------------------- ----------------- 210.7/385.0 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 211.2/385.0 MB 11.9 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 211.7/385.0 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 212.3/385.0 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 212.9/385.0 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 213.4/385.0 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 213.9/385.0 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 214.4/385.0 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 214.9/385.0 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 215.4/385.0 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 216.0/385.0 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 216.6/385.0 MB 11.5 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 217.2/385.0 MB 11.9 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 217.8/385.0 MB 11.9 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 218.2/385.0 MB 11.7 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 218.8/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 219.3/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 219.9/385.0 MB 11.7 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 220.4/385.0 MB 11.7 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 220.9/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 221.4/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 222.0/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 222.5/385.0 MB 11.7 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 222.9/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 223.4/385.0 MB 11.7 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 223.9/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 224.4/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 224.9/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 225.5/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 226.0/385.0 MB 11.7 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 226.5/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 227.1/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 227.5/385.0 MB 11.9 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 228.1/385.0 MB 12.1 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 228.5/385.0 MB 11.7 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 228.9/385.0 MB 11.5 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 229.5/385.0 MB 11.7 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 229.7/385.0 MB 11.7 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 230.2/385.0 MB 11.3 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 230.6/385.0 MB 11.1 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 231.2/385.0 MB 11.3 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 231.8/385.0 MB 11.5 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 232.3/385.0 MB 11.5 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 232.9/385.0 MB 11.3 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 233.4/385.0 MB 11.3 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 233.9/385.0 MB 11.5 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 234.2/385.0 MB 11.1 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 234.7/385.0 MB 11.1 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 235.3/385.0 MB 11.1 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 235.9/385.0 MB 11.3 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 236.3/385.0 MB 11.1 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 236.8/385.0 MB 11.1 MB/s eta 0:00:14\n",
      "   ------------------------ -------------- 237.4/385.0 MB 11.1 MB/s eta 0:00:14\n",
      "   ------------------------ -------------- 237.8/385.0 MB 11.1 MB/s eta 0:00:14\n",
      "   ------------------------ -------------- 238.3/385.0 MB 10.9 MB/s eta 0:00:14\n",
      "   ------------------------ -------------- 238.9/385.0 MB 11.3 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 239.5/385.0 MB 11.3 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 240.0/385.0 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 240.5/385.0 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 240.9/385.0 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 241.4/385.0 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 241.9/385.0 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 242.4/385.0 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 242.9/385.0 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 243.3/385.0 MB 11.5 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 243.5/385.0 MB 11.5 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 243.5/385.0 MB 11.5 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 244.9/385.0 MB 11.9 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 245.4/385.0 MB 11.9 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 245.8/385.0 MB 11.7 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 246.3/385.0 MB 11.7 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 246.9/385.0 MB 11.5 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 247.1/385.0 MB 11.5 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 247.7/385.0 MB 11.5 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 248.0/385.0 MB 11.5 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 248.3/385.0 MB 10.9 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 248.9/385.0 MB 10.9 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 249.5/385.0 MB 10.9 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 249.9/385.0 MB 10.7 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 250.5/385.0 MB 10.6 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 251.0/385.0 MB 10.7 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 251.3/385.0 MB 10.4 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 251.7/385.0 MB 10.4 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 252.2/385.0 MB 10.4 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 252.8/385.0 MB 10.4 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 253.3/385.0 MB 10.4 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 253.9/385.0 MB 11.3 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 254.4/385.0 MB 10.7 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 255.0/385.0 MB 10.4 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 255.6/385.0 MB 10.4 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 255.7/385.0 MB 10.1 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 256.2/385.0 MB 9.9 MB/s eta 0:00:14\n",
      "   ------------------------- ------------- 256.7/385.0 MB 10.1 MB/s eta 0:00:13\n",
      "   -------------------------- ------------ 257.3/385.0 MB 10.2 MB/s eta 0:00:13\n",
      "   -------------------------- ------------ 257.8/385.0 MB 10.2 MB/s eta 0:00:13\n",
      "   -------------------------- ------------ 258.4/385.0 MB 10.7 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 259.0/385.0 MB 10.7 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 259.5/385.0 MB 10.6 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 260.1/385.0 MB 10.9 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 260.5/385.0 MB 10.9 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 260.9/385.0 MB 10.9 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 261.5/385.0 MB 11.1 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 262.0/385.0 MB 11.1 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 262.5/385.0 MB 10.9 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 263.0/385.0 MB 10.9 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 263.6/385.0 MB 11.1 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 263.9/385.0 MB 11.1 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 264.5/385.0 MB 11.3 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 265.0/385.0 MB 11.1 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 265.6/385.0 MB 11.1 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 266.2/385.0 MB 11.7 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 266.7/385.0 MB 11.7 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 267.3/385.0 MB 11.7 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 267.8/385.0 MB 11.7 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 268.3/385.0 MB 11.7 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 268.9/385.0 MB 11.9 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 269.4/385.0 MB 11.7 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 269.8/385.0 MB 11.7 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 270.4/385.0 MB 11.7 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 270.7/385.0 MB 11.7 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 270.9/385.0 MB 11.1 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 271.5/385.0 MB 11.3 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 272.0/385.0 MB 11.1 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 272.7/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 273.1/385.0 MB 11.1 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 273.6/385.0 MB 11.1 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 274.2/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 274.7/385.0 MB 11.1 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 275.2/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 275.8/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 276.3/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 276.9/385.0 MB 10.9 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 277.4/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 277.9/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 278.5/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 278.9/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 279.6/385.0 MB 11.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 280.1/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 280.6/385.0 MB 11.3 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 281.1/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 281.7/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 282.3/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 282.7/385.0 MB 11.7 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 283.3/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 283.7/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 284.3/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 284.8/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 285.5/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 286.0/385.0 MB 12.1 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 286.5/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 287.0/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 287.4/385.0 MB 11.7 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 288.0/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 288.4/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 289.0/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 289.4/385.0 MB 11.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 290.0/385.0 MB 11.9 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 290.6/385.0 MB 11.9 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 291.1/385.0 MB 11.9 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 291.5/385.0 MB 11.7 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 292.2/385.0 MB 11.9 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 292.8/385.0 MB 11.9 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 293.3/385.0 MB 11.9 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 293.9/385.0 MB 12.1 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 294.5/385.0 MB 11.9 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 294.7/385.0 MB 11.7 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 295.3/385.0 MB 11.7 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 295.7/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 296.3/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 296.9/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 297.4/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 298.0/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 298.5/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 299.0/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 299.5/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 300.1/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 300.6/385.0 MB 11.7 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 301.1/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 301.7/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 302.2/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 302.8/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 303.4/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 303.9/385.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 304.5/385.0 MB 11.3 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 305.0/385.0 MB 11.9 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 305.6/385.0 MB 11.9 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 306.2/385.0 MB 11.9 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 306.7/385.0 MB 11.7 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 307.1/385.0 MB 11.5 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 307.6/385.0 MB 11.7 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 308.1/385.0 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 308.5/385.0 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 309.1/385.0 MB 11.5 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 309.5/385.0 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 310.1/385.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 310.6/385.0 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 311.1/385.0 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 311.7/385.0 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 312.0/385.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 312.5/385.0 MB 10.9 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 313.1/385.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 313.8/385.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 314.3/385.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 314.9/385.0 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 315.5/385.0 MB 11.3 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 316.1/385.0 MB 11.1 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 316.6/385.0 MB 11.1 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 317.2/385.0 MB 11.3 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 317.7/385.0 MB 11.3 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 318.2/385.0 MB 11.5 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 318.7/385.0 MB 11.5 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 319.3/385.0 MB 11.5 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 319.8/385.0 MB 11.9 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 320.3/385.0 MB 11.7 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 320.9/385.0 MB 11.7 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 321.5/385.0 MB 11.7 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 322.0/385.0 MB 11.7 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 322.6/385.0 MB 12.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 323.1/385.0 MB 12.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 323.7/385.0 MB 11.9 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 324.2/385.0 MB 11.9 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 324.8/385.0 MB 11.9 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 325.4/385.0 MB 11.9 MB/s eta 0:00:06\n",
      "   --------------------------------- ----- 325.9/385.0 MB 11.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 326.5/385.0 MB 11.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 327.1/385.0 MB 11.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 327.6/385.0 MB 11.7 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 328.2/385.0 MB 11.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 328.7/385.0 MB 11.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 329.3/385.0 MB 11.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 329.9/385.0 MB 11.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 330.3/385.0 MB 11.7 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 330.8/385.0 MB 11.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 330.9/385.0 MB 11.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 331.4/385.0 MB 11.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 331.9/385.0 MB 11.3 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 332.6/385.0 MB 11.3 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 333.1/385.0 MB 11.3 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 333.7/385.0 MB 11.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 334.2/385.0 MB 11.3 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 334.8/385.0 MB 11.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 335.3/385.0 MB 11.3 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 335.9/385.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 336.4/385.0 MB 11.5 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 337.0/385.0 MB 11.5 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 337.6/385.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 338.1/385.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 338.7/385.0 MB 11.3 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 339.2/385.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 339.8/385.0 MB 11.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 340.4/385.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 340.9/385.0 MB 11.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 341.4/385.0 MB 11.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 341.8/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 342.3/385.0 MB 11.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 342.8/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 343.3/385.0 MB 11.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 343.8/385.0 MB 11.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 344.4/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 344.9/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 345.4/385.0 MB 11.9 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 345.8/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 346.4/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 347.0/385.0 MB 11.9 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 347.5/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 348.1/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 348.7/385.0 MB 11.9 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 349.2/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 349.8/385.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 350.4/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 350.8/385.0 MB 11.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 351.3/385.0 MB 11.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 351.9/385.0 MB 11.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 352.5/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 352.9/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 353.5/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 354.0/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 354.4/385.0 MB 11.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 354.9/385.0 MB 11.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 355.4/385.0 MB 11.9 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 355.9/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 356.5/385.0 MB 11.9 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 357.0/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 357.6/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 358.2/385.0 MB 11.9 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 358.7/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 359.2/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 359.7/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 360.3/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 360.8/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 361.2/385.0 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 361.7/385.0 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 362.3/385.0 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 362.7/385.0 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 363.3/385.0 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 363.9/385.0 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 364.4/385.0 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 364.9/385.0 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 365.4/385.0 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 366.0/385.0 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 366.5/385.0 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 367.0/385.0 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 367.2/385.0 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 367.5/385.0 MB 11.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 368.2/385.0 MB 11.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 368.3/385.0 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 368.8/385.0 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 369.3/385.0 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 369.9/385.0 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 370.3/385.0 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 370.8/385.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 371.4/385.0 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 372.0/385.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 372.5/385.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 372.9/385.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 373.5/385.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 374.0/385.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 374.6/385.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.2/385.0 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.6/385.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  376.0/385.0 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  376.6/385.0 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.2/385.0 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.7/385.0 MB 11.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  378.2/385.0 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  378.8/385.0 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  379.3/385.0 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  379.9/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  380.4/385.0 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  380.9/385.0 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.5/385.0 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.1/385.0 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.6/385.0 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  383.2/385.0 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  383.7/385.0 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.3/385.0 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.8/385.0 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 385.0/385.0 MB 6.5 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.7/133.7 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.66.1-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.6/4.3 MB 17.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.1/4.3 MB 14.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.7/4.3 MB 13.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.2/4.3 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.7/4.3 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.1/4.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.7/4.3 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.0 MB 15.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.1/3.0 MB 11.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.6/3.0 MB 12.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.0/3.0 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.5/3.0 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/3.0 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.6/1.1 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.1/1.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 8.1 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/26.4 MB 13.8 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 1.0/26.4 MB 15.1 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.5/26.4 MB 12.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 2.0/26.4 MB 12.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 2.6/26.4 MB 12.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 3.1/26.4 MB 12.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.7/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 4.3/26.4 MB 12.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.8/26.4 MB 12.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.4/26.4 MB 12.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.0/26.4 MB 12.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.5/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 7.1/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.6/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 8.2/26.4 MB 12.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.8/26.4 MB 12.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 9.4/26.4 MB 12.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.9/26.4 MB 12.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 10.5/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.0/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.6/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.2/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.7/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.3/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.7/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 14.3/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.9/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.4/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.9/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 16.5/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 17.0/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 17.6/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 18.2/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.7/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.3/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.9/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 20.4/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 21.0/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.5/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 22.1/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.7/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.2/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.8/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.3/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.9/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.5/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.1/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 10.9 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.0-cp311-cp311-win_amd64.whl (126 kB)\n",
      "   ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 126.8/126.8 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.5 MB 17.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.1/5.5 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.7/5.5 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.2/5.5 MB 12.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.8/5.5 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.5 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.9/5.5 MB 12.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.5 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.5 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 16.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 268.5/268.5 kB 16.1 MB/s eta 0:00:00\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.66.1 h5py-3.11.0 keras-3.5.0 libclang-18.1.1 ml-dtypes-0.4.0 namex-0.0.8 opt-einsum-3.3.0 optree-0.12.1 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-intel-2.17.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df63e9-0a2b-488b-9a09-bd986e044638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47a500-fe83-45fc-a1af-79a37cbcb34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec30adb-8f15-4958-9915-26c034d8ac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customerID                                object\n",
      "SeniorCitizen                              int64\n",
      "tenure                                   float64\n",
      "MonthlyCharges                           float64\n",
      "TotalCharges                             float64\n",
      "gender_Male                                 bool\n",
      "Partner_Yes                                 bool\n",
      "Dependents_Yes                              bool\n",
      "PhoneService_Yes                            bool\n",
      "MultipleLines_No phone service              bool\n",
      "MultipleLines_Yes                           bool\n",
      "InternetService_Fiber optic                 bool\n",
      "InternetService_No                          bool\n",
      "OnlineSecurity_No internet service          bool\n",
      "OnlineSecurity_Yes                          bool\n",
      "OnlineBackup_No internet service            bool\n",
      "OnlineBackup_Yes                            bool\n",
      "DeviceProtection_No internet service        bool\n",
      "DeviceProtection_Yes                        bool\n",
      "TechSupport_No internet service             bool\n",
      "TechSupport_Yes                             bool\n",
      "StreamingTV_No internet service             bool\n",
      "StreamingTV_Yes                             bool\n",
      "StreamingMovies_No internet service         bool\n",
      "StreamingMovies_Yes                         bool\n",
      "Contract_One year                           bool\n",
      "Contract_Two year                           bool\n",
      "PaperlessBilling_Yes                        bool\n",
      "PaymentMethod_Credit card (automatic)       bool\n",
      "PaymentMethod_Electronic check              bool\n",
      "PaymentMethod_Mailed check                  bool\n",
      "Churn_Yes                                   bool\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6950 - loss: 0.6159 - val_accuracy: 0.7799 - val_loss: 0.4522\n",
      "Epoch 2/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7800 - loss: 0.4667 - val_accuracy: 0.7728 - val_loss: 0.4405\n",
      "Epoch 3/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8017 - loss: 0.4535 - val_accuracy: 0.7915 - val_loss: 0.4370\n",
      "Epoch 4/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8250 - loss: 0.3948 - val_accuracy: 0.7906 - val_loss: 0.4410\n",
      "Epoch 5/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8900 - loss: 0.3017 - val_accuracy: 0.7773 - val_loss: 0.4504\n",
      "Epoch 6/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9629 - loss: 0.1457 - val_accuracy: 0.7578 - val_loss: 0.4684\n",
      "Epoch 7/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9943 - loss: 0.0548 - val_accuracy: 0.7560 - val_loss: 0.4805\n",
      "Epoch 8/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9987 - loss: 0.0238 - val_accuracy: 0.7720 - val_loss: 0.4670\n",
      "Epoch 9/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9991 - loss: 0.0149 - val_accuracy: 0.7622 - val_loss: 0.4666\n",
      "Epoch 10/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 0.7471 - val_loss: 0.4844\n",
      "Epoch 11/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0069 - val_accuracy: 0.7666 - val_loss: 0.4625\n",
      "Epoch 12/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.7613 - val_loss: 0.4678\n",
      "Epoch 13/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.7542 - val_loss: 0.4783\n",
      "Epoch 14/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7578 - val_loss: 0.4879\n",
      "Epoch 15/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7516 - val_loss: 0.4982\n",
      "Epoch 16/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 0.7595 - val_loss: 0.4870\n",
      "Epoch 17/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7746 - val_loss: 0.4823\n",
      "Epoch 18/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7604 - val_loss: 0.5081\n",
      "Epoch 19/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7489 - val_loss: 0.5308\n",
      "Epoch 20/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7640 - val_loss: 0.4851\n",
      "Epoch 21/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7462 - val_loss: 0.5167\n",
      "Epoch 22/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 8.7263e-04 - val_accuracy: 0.7587 - val_loss: 0.5655\n",
      "Epoch 23/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 8.9386e-04 - val_accuracy: 0.7409 - val_loss: 0.5239\n",
      "Epoch 24/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9998 - loss: 9.0980e-04 - val_accuracy: 0.7214 - val_loss: 0.6530\n",
      "Epoch 25/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7098 - val_loss: 0.5481\n",
      "Epoch 26/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0012 - val_accuracy: 0.6921 - val_loss: 0.7157\n",
      "Epoch 27/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 9.4352e-04 - val_accuracy: 0.7400 - val_loss: 0.5043\n",
      "Epoch 28/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7072 - val_loss: 0.6175\n",
      "Epoch 29/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.5413e-04 - val_accuracy: 0.6983 - val_loss: 0.6436\n",
      "Epoch 30/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.2890e-04 - val_accuracy: 0.7489 - val_loss: 0.5136\n",
      "Epoch 31/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 8.6609e-04 - val_accuracy: 0.7072 - val_loss: 0.6129\n",
      "Epoch 32/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.3931e-04 - val_accuracy: 0.7320 - val_loss: 0.5516\n",
      "Epoch 33/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.8282e-04 - val_accuracy: 0.7276 - val_loss: 0.5938\n",
      "Epoch 34/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.7449e-04 - val_accuracy: 0.7445 - val_loss: 0.5066\n",
      "Epoch 35/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.5446e-04 - val_accuracy: 0.7258 - val_loss: 0.5557\n",
      "Epoch 36/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3728e-04 - val_accuracy: 0.7489 - val_loss: 0.5137\n",
      "Epoch 37/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.5750 - val_loss: 1.1900\n",
      "Epoch 38/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9940 - loss: 0.0183 - val_accuracy: 0.4357 - val_loss: 1.2539\n",
      "Epoch 39/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.5684e-04 - val_accuracy: 0.6371 - val_loss: 0.7439\n",
      "Epoch 40/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 6.1988e-04 - val_accuracy: 0.6903 - val_loss: 0.6943\n",
      "Epoch 41/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.3987e-04 - val_accuracy: 0.7063 - val_loss: 0.6180\n",
      "Epoch 42/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.7137e-04 - val_accuracy: 0.7045 - val_loss: 0.6356\n",
      "Epoch 43/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.0522e-04 - val_accuracy: 0.7347 - val_loss: 0.5810\n",
      "Epoch 44/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.5787e-04 - val_accuracy: 0.7116 - val_loss: 0.6433\n",
      "Epoch 45/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.2304e-04 - val_accuracy: 0.7036 - val_loss: 0.6578\n",
      "Epoch 46/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 9.6722e-04 - val_accuracy: 0.7125 - val_loss: 0.6332\n",
      "Epoch 47/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.1827e-04 - val_accuracy: 0.6477 - val_loss: 1.2147\n",
      "Epoch 48/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.2022e-04 - val_accuracy: 0.7258 - val_loss: 0.5676\n",
      "Epoch 49/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.3241e-04 - val_accuracy: 0.7098 - val_loss: 0.6080\n",
      "Epoch 50/50\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.4470e-04 - val_accuracy: 0.7347 - val_loss: 0.5552\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7745 - loss: 0.5475\n",
      "Test Accuracy: 0.7572746872901917\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('D02fully_processed_telco_data.csv')\n",
    "\n",
    "# Check for any non-numeric columns that need to be encoded or removed\n",
    "print(df.dtypes)  # This will help in identifying non-numeric columns\n",
    "\n",
    "# Assuming 'Churn_Yes' is the target\n",
    "X = df.drop('Churn_Yes', axis=1)\n",
    "y = df['Churn_Yes'].values\n",
    "\n",
    "# Automatically identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define a pipeline to transform data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Applying transformations\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural Network Model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc180842-ff44-4b65-9df5-5f8b08a75030",
   "metadata": {},
   "source": [
    "# explaination of the code \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3900958-5c68-4c68-bf06-3bc0d0236dc3",
   "metadata": {},
   "source": [
    "Certainly! The script provided is a comprehensive setup for preparing your data and training a neural network to predict customer churn. Here’s a detailed breakdown of each part of the script:\n",
    "\n",
    "### Data Loading and Initial Processing\n",
    "\n",
    "```python\n",
    "# Load data\n",
    "df = pd.read_csv('/path/to/fully_processed_telco_data.csv')\n",
    "\n",
    "# Check for any non-numeric columns that need to be encoded or removed\n",
    "print(df.dtypes)  # This will help in identifying non-numeric columns\n",
    "```\n",
    "\n",
    "- **Data Loading:** The data is loaded from a CSV file into a pandas DataFrame.\n",
    "- **Data Inspection:** The `dtypes` method is used to print the data types of each column in the DataFrame. This helps identify columns that are not numeric and need further preprocessing before they can be used in the neural network.\n",
    "\n",
    "### Feature Selection and Preprocessing\n",
    "\n",
    "```python\n",
    "# Assuming 'Churn_Yes' is the target\n",
    "X = df.drop('Churn_Yes', axis=1)\n",
    "y = df['Churn_Yes'].values\n",
    "```\n",
    "\n",
    "- **Feature and Target Separation:** `X` contains the features (independent variables), and `y` contains the target variable (dependent variable, 'Churn_Yes').\n",
    "\n",
    "```python\n",
    "# Automatically identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "```\n",
    "\n",
    "- **Identify Column Types:** Automatically identifies which columns are numeric and which are categorical. This categorization is crucial because numeric and categorical data require different types of preprocessing.\n",
    "\n",
    "### Defining the Transformation Pipelines\n",
    "\n",
    "```python\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "```\n",
    "\n",
    "- **Numeric Transformer:** A pipeline for numeric features which includes:\n",
    "  - **Imputation:** Fills missing values with the median of each column.\n",
    "  - **Scaling:** Standardizes features by removing the mean and scaling to unit variance.\n",
    "  \n",
    "- **Categorical Transformer:** A pipeline for categorical features which includes:\n",
    "  - **Imputation:** Fills missing values with a constant ('missing').\n",
    "  - **OneHot Encoding:** Transforms categorical variables into a form that could be provided to ML algorithms to do a better job in prediction.\n",
    "\n",
    "```python\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "```\n",
    "\n",
    "- **Column Transformer:** Combines the two pipelines into a single transformer that applies the appropriate transformations to each column type.\n",
    "\n",
    "### Applying Transformations and Splitting Data\n",
    "\n",
    "```python\n",
    "# Applying transformations\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "```\n",
    "\n",
    "- **Apply Preprocessor:** The `fit_transform` method fits the transformation to the data and then transforms it. This prepares the data for modeling.\n",
    "\n",
    "```python\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "- **Data Splitting:** Divides the data into training and testing sets to ensure the model can be trained and then independently evaluated.\n",
    "\n",
    "### Building and Training the Neural Network\n",
    "\n",
    "```python\n",
    "# Neural Network Model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "```\n",
    "\n",
    "- **Neural Network Configuration:** Constructs a sequential neural network with:\n",
    "  - **Dense Layers:** Fully connected layers with `relu` activation for hidden layers and `sigmoid` for the output layer (suitable for binary classification).\n",
    "  - **Dropout:** Used to prevent overfitting by randomly setting a fraction of the input units to 0 at each update during training.\n",
    "- **Compilation:** Configures the model for training with the Adam optimizer and binary crossentropy as the loss function.\n",
    "- **Training:** Fits the model on the training data while also validating on a portion of it.\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "```python\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "```\n",
    "\n",
    "- **Evaluation:** Measures the model's performance on the unseen test data and prints the accuracy.\n",
    "\n",
    "This setup is comprehensive, integrating data preprocessing with model configuration and evaluation, ensuring that the neural network is appropriately fed with preprocessed data for churn prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c44ab4-8716-4043-9e1f-8346e766742a",
   "metadata": {},
   "source": [
    "# we just test he neural with epoch 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98e0374d-b3b2-4794-863d-493ee0857d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customerID                                object\n",
      "SeniorCitizen                              int64\n",
      "tenure                                   float64\n",
      "MonthlyCharges                           float64\n",
      "TotalCharges                             float64\n",
      "gender_Male                                 bool\n",
      "Partner_Yes                                 bool\n",
      "Dependents_Yes                              bool\n",
      "PhoneService_Yes                            bool\n",
      "MultipleLines_No phone service              bool\n",
      "MultipleLines_Yes                           bool\n",
      "InternetService_Fiber optic                 bool\n",
      "InternetService_No                          bool\n",
      "OnlineSecurity_No internet service          bool\n",
      "OnlineSecurity_Yes                          bool\n",
      "OnlineBackup_No internet service            bool\n",
      "OnlineBackup_Yes                            bool\n",
      "DeviceProtection_No internet service        bool\n",
      "DeviceProtection_Yes                        bool\n",
      "TechSupport_No internet service             bool\n",
      "TechSupport_Yes                             bool\n",
      "StreamingTV_No internet service             bool\n",
      "StreamingTV_Yes                             bool\n",
      "StreamingMovies_No internet service         bool\n",
      "StreamingMovies_Yes                         bool\n",
      "Contract_One year                           bool\n",
      "Contract_Two year                           bool\n",
      "PaperlessBilling_Yes                        bool\n",
      "PaymentMethod_Credit card (automatic)       bool\n",
      "PaymentMethod_Electronic check              bool\n",
      "PaymentMethod_Mailed check                  bool\n",
      "Churn_Yes                                   bool\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\cprogramfiles\\anaconda2\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7094 - loss: 0.6046 - val_accuracy: 0.7613 - val_loss: 0.4521\n",
      "Epoch 2/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7786 - loss: 0.4661 - val_accuracy: 0.7622 - val_loss: 0.4428\n",
      "Epoch 3/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7964 - loss: 0.4491 - val_accuracy: 0.7666 - val_loss: 0.4384\n",
      "Epoch 4/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8365 - loss: 0.3931 - val_accuracy: 0.7799 - val_loss: 0.4392\n",
      "Epoch 5/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8948 - loss: 0.2874 - val_accuracy: 0.7826 - val_loss: 0.4397\n",
      "Epoch 6/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9753 - loss: 0.1311 - val_accuracy: 0.7737 - val_loss: 0.4599\n",
      "Epoch 7/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9963 - loss: 0.0493 - val_accuracy: 0.7666 - val_loss: 0.4577\n",
      "Epoch 8/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9991 - loss: 0.0221 - val_accuracy: 0.7675 - val_loss: 0.4553\n",
      "Epoch 9/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9998 - loss: 0.0121 - val_accuracy: 0.7338 - val_loss: 0.5132\n",
      "Epoch 10/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9991 - loss: 0.0092 - val_accuracy: 0.7569 - val_loss: 0.4679\n",
      "Epoch 11/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9996 - loss: 0.0059 - val_accuracy: 0.7693 - val_loss: 0.4645\n",
      "Epoch 12/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.7240 - val_loss: 0.4982\n",
      "Epoch 13/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.7702 - val_loss: 0.4740\n",
      "Epoch 14/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7578 - val_loss: 0.4940\n",
      "Epoch 15/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.7533 - val_loss: 0.4799\n",
      "Epoch 16/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7409 - val_loss: 0.5374\n",
      "Epoch 17/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7551 - val_loss: 0.5023\n",
      "Epoch 18/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7542 - val_loss: 0.4885\n",
      "Epoch 19/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7533 - val_loss: 0.4956\n",
      "Epoch 20/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9993 - loss: 0.0019 - val_accuracy: 0.7516 - val_loss: 0.5261\n",
      "Epoch 21/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7524 - val_loss: 0.5218\n",
      "Epoch 22/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7587 - val_loss: 0.5555\n",
      "Epoch 23/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 9.5807e-04 - val_accuracy: 0.7320 - val_loss: 0.6338\n",
      "Epoch 24/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.7453 - val_loss: 0.5548\n",
      "Epoch 25/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.3047e-04 - val_accuracy: 0.7578 - val_loss: 0.5332\n",
      "Epoch 26/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7311 - val_loss: 0.5688\n",
      "Epoch 27/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.4394e-04 - val_accuracy: 0.7249 - val_loss: 0.6140\n",
      "Epoch 28/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.5241e-04 - val_accuracy: 0.7604 - val_loss: 0.5034\n",
      "Epoch 29/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.5230e-04 - val_accuracy: 0.7125 - val_loss: 0.6648\n",
      "Epoch 30/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.0668e-04 - val_accuracy: 0.7232 - val_loss: 0.6816\n",
      "Epoch 31/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.3269e-04 - val_accuracy: 0.7152 - val_loss: 0.5677\n",
      "Epoch 32/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.7249 - val_loss: 0.7142\n",
      "Epoch 33/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9980 - loss: 0.0065 - val_accuracy: 0.6539 - val_loss: 0.7369\n",
      "Epoch 34/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 0.6823 - val_loss: 0.8028\n",
      "Epoch 35/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.5861e-04 - val_accuracy: 0.6868 - val_loss: 0.7754\n",
      "Epoch 36/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.8783e-04 - val_accuracy: 0.7001 - val_loss: 0.7005\n",
      "Epoch 37/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.5150e-04 - val_accuracy: 0.7178 - val_loss: 0.6660\n",
      "Epoch 38/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.9537e-04 - val_accuracy: 0.7063 - val_loss: 0.7579\n",
      "Epoch 39/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.0422e-04 - val_accuracy: 0.6965 - val_loss: 0.7725\n",
      "Epoch 40/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.1495e-04 - val_accuracy: 0.6957 - val_loss: 0.8637\n",
      "Epoch 41/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.7010 - val_loss: 0.7223\n",
      "Epoch 42/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.0356e-04 - val_accuracy: 0.7107 - val_loss: 0.7696\n",
      "Epoch 43/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.6455e-04 - val_accuracy: 0.6114 - val_loss: 1.1351\n",
      "Epoch 44/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.0201e-04 - val_accuracy: 0.7107 - val_loss: 0.6191\n",
      "Epoch 45/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.2821e-04 - val_accuracy: 0.7356 - val_loss: 0.6193\n",
      "Epoch 46/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.9767e-04 - val_accuracy: 0.6850 - val_loss: 0.6873\n",
      "Epoch 47/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.8503e-04 - val_accuracy: 0.6894 - val_loss: 0.6942\n",
      "Epoch 48/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5429e-04 - val_accuracy: 0.6362 - val_loss: 0.9304\n",
      "Epoch 49/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6344e-04 - val_accuracy: 0.6655 - val_loss: 0.6878\n",
      "Epoch 50/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.6290e-04 - val_accuracy: 0.6504 - val_loss: 0.8023\n",
      "Epoch 51/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.5609e-05 - val_accuracy: 0.6832 - val_loss: 0.7018\n",
      "Epoch 52/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.3746e-05 - val_accuracy: 0.6744 - val_loss: 0.8461\n",
      "Epoch 53/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6277e-04 - val_accuracy: 0.7010 - val_loss: 0.7040\n",
      "Epoch 54/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.2985e-04 - val_accuracy: 0.7178 - val_loss: 0.6812\n",
      "Epoch 55/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.5318e-05 - val_accuracy: 0.7196 - val_loss: 0.6814\n",
      "Epoch 56/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.6544e-04 - val_accuracy: 0.6992 - val_loss: 0.9385\n",
      "Epoch 57/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.7829e-05 - val_accuracy: 0.7054 - val_loss: 0.7962\n",
      "Epoch 58/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.9836e-05 - val_accuracy: 0.6850 - val_loss: 0.9507\n",
      "Epoch 59/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.5948e-04 - val_accuracy: 0.6894 - val_loss: 0.9627\n",
      "Epoch 60/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9976 - loss: 0.0083 - val_accuracy: 0.7196 - val_loss: 0.8453\n",
      "Epoch 61/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 0.7347 - val_loss: 0.8077\n",
      "Epoch 62/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.8684e-04 - val_accuracy: 0.6797 - val_loss: 0.8530\n",
      "Epoch 63/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6149e-04 - val_accuracy: 0.7010 - val_loss: 0.7960\n",
      "Epoch 64/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.1465e-04 - val_accuracy: 0.7054 - val_loss: 0.7919\n",
      "Epoch 65/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3229e-04 - val_accuracy: 0.7143 - val_loss: 0.7912\n",
      "Epoch 66/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6190e-04 - val_accuracy: 0.6406 - val_loss: 1.2523\n",
      "Epoch 67/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1546e-04 - val_accuracy: 0.6841 - val_loss: 0.9883\n",
      "Epoch 68/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.6224e-05 - val_accuracy: 0.6841 - val_loss: 0.9712\n",
      "Epoch 69/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0447e-04 - val_accuracy: 0.6859 - val_loss: 0.9654\n",
      "Epoch 70/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4100e-04 - val_accuracy: 0.6690 - val_loss: 1.0526\n",
      "Epoch 71/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.0217e-04 - val_accuracy: 0.7019 - val_loss: 0.8819\n",
      "Epoch 72/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.1812e-04 - val_accuracy: 0.7382 - val_loss: 0.6978\n",
      "Epoch 73/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.8930e-04 - val_accuracy: 0.6957 - val_loss: 0.7500\n",
      "Epoch 74/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.8388e-05 - val_accuracy: 0.6238 - val_loss: 0.9965\n",
      "Epoch 75/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1366e-04 - val_accuracy: 0.6575 - val_loss: 0.8957\n",
      "Epoch 76/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.2078e-05 - val_accuracy: 0.6806 - val_loss: 0.8764\n",
      "Epoch 77/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.8616e-04 - val_accuracy: 0.7152 - val_loss: 0.8119\n",
      "Epoch 78/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.4179e-04 - val_accuracy: 0.7134 - val_loss: 0.8286\n",
      "Epoch 79/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.1597e-05 - val_accuracy: 0.7036 - val_loss: 0.8984\n",
      "Epoch 80/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.5395e-05 - val_accuracy: 0.7098 - val_loss: 0.8688\n",
      "Epoch 81/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.0322e-04 - val_accuracy: 0.7276 - val_loss: 0.8343\n",
      "Epoch 82/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.4858e-05 - val_accuracy: 0.7107 - val_loss: 0.8937\n",
      "Epoch 83/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.7132e-04 - val_accuracy: 0.6894 - val_loss: 0.8281\n",
      "Epoch 84/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.2284e-05 - val_accuracy: 0.6965 - val_loss: 0.8313\n",
      "Epoch 85/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.7298e-05 - val_accuracy: 0.7152 - val_loss: 0.7742\n",
      "Epoch 86/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.5480e-05 - val_accuracy: 0.7187 - val_loss: 0.7681\n",
      "Epoch 87/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.1384e-05 - val_accuracy: 0.6424 - val_loss: 1.4075\n",
      "Epoch 88/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.6877e-05 - val_accuracy: 0.6291 - val_loss: 1.5240\n",
      "Epoch 89/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.7056e-05 - val_accuracy: 0.6513 - val_loss: 1.2900\n",
      "Epoch 90/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.6831e-05 - val_accuracy: 0.6486 - val_loss: 1.3226\n",
      "Epoch 91/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.5838e-05 - val_accuracy: 0.6122 - val_loss: 1.3158\n",
      "Epoch 92/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.8324e-05 - val_accuracy: 0.6673 - val_loss: 1.0824\n",
      "Epoch 93/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.2359e-04 - val_accuracy: 0.7374 - val_loss: 0.5671\n",
      "Epoch 94/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4543e-05 - val_accuracy: 0.7347 - val_loss: 0.5897\n",
      "Epoch 95/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.6956e-05 - val_accuracy: 0.7161 - val_loss: 0.6549\n",
      "Epoch 96/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.3296e-04 - val_accuracy: 0.6220 - val_loss: 1.0081\n",
      "Epoch 97/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.0226e-05 - val_accuracy: 0.7090 - val_loss: 0.7067\n",
      "Epoch 98/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.6528e-05 - val_accuracy: 0.6965 - val_loss: 0.7582\n",
      "Epoch 99/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.1170e-05 - val_accuracy: 0.6681 - val_loss: 0.8855\n",
      "Epoch 100/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.4558e-05 - val_accuracy: 0.6389 - val_loss: 1.3588\n",
      "Epoch 101/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.4320e-05 - val_accuracy: 0.5980 - val_loss: 2.2046\n",
      "Epoch 102/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.8094e-04 - val_accuracy: 0.6309 - val_loss: 1.4791\n",
      "Epoch 103/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.8609e-05 - val_accuracy: 0.6504 - val_loss: 1.1759\n",
      "Epoch 104/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.7619e-05 - val_accuracy: 0.6371 - val_loss: 1.6115\n",
      "Epoch 105/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6962e-05 - val_accuracy: 0.7098 - val_loss: 0.9563\n",
      "Epoch 106/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9996 - loss: 6.6080e-04 - val_accuracy: 0.7560 - val_loss: 1.4490\n",
      "Epoch 107/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9978 - loss: 0.0057 - val_accuracy: 0.6965 - val_loss: 1.7482\n",
      "Epoch 108/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0012 - val_accuracy: 0.5661 - val_loss: 1.7464\n",
      "Epoch 109/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 7.7503e-04 - val_accuracy: 0.6619 - val_loss: 0.9719\n",
      "Epoch 110/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 9.7877e-04 - val_accuracy: 0.7666 - val_loss: 0.8434\n",
      "Epoch 111/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.8987e-04 - val_accuracy: 0.7560 - val_loss: 0.7897\n",
      "Epoch 112/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.3186e-05 - val_accuracy: 0.6584 - val_loss: 1.5442\n",
      "Epoch 113/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.0649e-04 - val_accuracy: 0.6575 - val_loss: 1.5875\n",
      "Epoch 114/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.9670e-05 - val_accuracy: 0.6593 - val_loss: 1.5736\n",
      "Epoch 115/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.7636e-05 - val_accuracy: 0.6602 - val_loss: 1.5092\n",
      "Epoch 116/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.2756e-05 - val_accuracy: 0.6726 - val_loss: 1.4285\n",
      "Epoch 117/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.9436e-04 - val_accuracy: 0.6815 - val_loss: 1.3230\n",
      "Epoch 118/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.6522e-05 - val_accuracy: 0.6779 - val_loss: 1.3112\n",
      "Epoch 119/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.9770e-05 - val_accuracy: 0.6877 - val_loss: 1.2569\n",
      "Epoch 120/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.9317e-05 - val_accuracy: 0.6930 - val_loss: 1.2273\n",
      "Epoch 121/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.8789e-05 - val_accuracy: 0.7054 - val_loss: 1.1569\n",
      "Epoch 122/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.2082e-05 - val_accuracy: 0.7028 - val_loss: 1.1440\n",
      "Epoch 123/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.3474e-05 - val_accuracy: 0.7010 - val_loss: 1.1447\n",
      "Epoch 124/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.8752e-05 - val_accuracy: 0.6948 - val_loss: 1.1661\n",
      "Epoch 125/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.9437e-05 - val_accuracy: 0.6965 - val_loss: 1.1722\n",
      "Epoch 126/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.2140e-04 - val_accuracy: 0.7010 - val_loss: 1.1643\n",
      "Epoch 127/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.5459e-04 - val_accuracy: 0.7036 - val_loss: 1.1152\n",
      "Epoch 128/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.7066e-04 - val_accuracy: 0.7098 - val_loss: 1.0898\n",
      "Epoch 129/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 5.6906e-05 - val_accuracy: 0.7267 - val_loss: 1.5269\n",
      "Epoch 130/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.6355e-04 - val_accuracy: 0.7240 - val_loss: 1.0759\n",
      "Epoch 131/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.9861e-04 - val_accuracy: 0.7320 - val_loss: 0.9881\n",
      "Epoch 132/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.3801e-05 - val_accuracy: 0.7311 - val_loss: 0.9839\n",
      "Epoch 133/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.2642e-04 - val_accuracy: 0.7356 - val_loss: 0.9693\n",
      "Epoch 134/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1619e-04 - val_accuracy: 0.7356 - val_loss: 0.9506\n",
      "Epoch 135/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1082e-04 - val_accuracy: 0.7382 - val_loss: 0.9301\n",
      "Epoch 136/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.8609e-04 - val_accuracy: 0.7418 - val_loss: 0.8782\n",
      "Epoch 137/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.8598e-05 - val_accuracy: 0.7533 - val_loss: 0.8732\n",
      "Epoch 138/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.0331e-06 - val_accuracy: 0.7595 - val_loss: 0.8630\n",
      "Epoch 139/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.9828e-05 - val_accuracy: 0.7542 - val_loss: 0.8786\n",
      "Epoch 140/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5667e-04 - val_accuracy: 0.7560 - val_loss: 0.8891\n",
      "Epoch 141/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.0493e-06 - val_accuracy: 0.7560 - val_loss: 0.8913\n",
      "Epoch 142/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.2536e-05 - val_accuracy: 0.7533 - val_loss: 0.8839\n",
      "Epoch 143/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.9336e-04 - val_accuracy: 0.7533 - val_loss: 0.8672\n",
      "Epoch 144/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.1600e-06 - val_accuracy: 0.7542 - val_loss: 0.8777\n",
      "Epoch 145/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3915e-05 - val_accuracy: 0.7551 - val_loss: 0.8756\n",
      "Epoch 146/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.9549e-06 - val_accuracy: 0.7587 - val_loss: 0.8771\n",
      "Epoch 147/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.7949e-04 - val_accuracy: 0.7613 - val_loss: 0.8879\n",
      "Epoch 148/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.4726e-06 - val_accuracy: 0.7613 - val_loss: 0.8803\n",
      "Epoch 149/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.9113e-05 - val_accuracy: 0.7631 - val_loss: 0.8211\n",
      "Epoch 150/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.3303e-05 - val_accuracy: 0.7613 - val_loss: 0.8063\n",
      "Epoch 151/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.9568e-04 - val_accuracy: 0.7613 - val_loss: 0.8425\n",
      "Epoch 152/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.1018e-06 - val_accuracy: 0.7640 - val_loss: 0.8216\n",
      "Epoch 153/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.7291e-06 - val_accuracy: 0.7649 - val_loss: 0.8359\n",
      "Epoch 154/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.1559e-05 - val_accuracy: 0.7631 - val_loss: 0.8548\n",
      "Epoch 155/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4123e-05 - val_accuracy: 0.7560 - val_loss: 0.8437\n",
      "Epoch 156/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.6213e-06 - val_accuracy: 0.7569 - val_loss: 0.8623\n",
      "Epoch 157/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.1151e-05 - val_accuracy: 0.7551 - val_loss: 0.8774\n",
      "Epoch 158/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1125e-05 - val_accuracy: 0.7604 - val_loss: 0.9122\n",
      "Epoch 159/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.2848e-05 - val_accuracy: 0.7604 - val_loss: 0.9192\n",
      "Epoch 160/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.3224e-06 - val_accuracy: 0.7595 - val_loss: 0.9430\n",
      "Epoch 161/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.0365e-05 - val_accuracy: 0.7143 - val_loss: 2.6858\n",
      "Epoch 162/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.8693e-05 - val_accuracy: 0.7232 - val_loss: 1.9970\n",
      "Epoch 163/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.0704e-06 - val_accuracy: 0.7418 - val_loss: 1.7490\n",
      "Epoch 164/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.7433e-05 - val_accuracy: 0.7436 - val_loss: 1.7096\n",
      "Epoch 165/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.2778e-04 - val_accuracy: 0.7462 - val_loss: 1.5029\n",
      "Epoch 166/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1316e-06 - val_accuracy: 0.7462 - val_loss: 1.4509\n",
      "Epoch 167/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.1962e-06 - val_accuracy: 0.7436 - val_loss: 1.2440\n",
      "Epoch 168/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.9662e-07 - val_accuracy: 0.7471 - val_loss: 1.2041\n",
      "Epoch 169/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9997 - loss: 7.1718e-04 - val_accuracy: 0.6575 - val_loss: 2.1522\n",
      "Epoch 170/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0026 - val_accuracy: 0.7347 - val_loss: 1.0404\n",
      "Epoch 171/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 0.7445 - val_loss: 0.9899\n",
      "Epoch 172/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1205e-05 - val_accuracy: 0.7462 - val_loss: 0.8907\n",
      "Epoch 173/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.0867e-05 - val_accuracy: 0.7507 - val_loss: 0.7991\n",
      "Epoch 174/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4512e-04 - val_accuracy: 0.7498 - val_loss: 0.8159\n",
      "Epoch 175/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.7771e-05 - val_accuracy: 0.7498 - val_loss: 0.8246\n",
      "Epoch 176/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0926e-04 - val_accuracy: 0.7489 - val_loss: 0.9962\n",
      "Epoch 177/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1873e-05 - val_accuracy: 0.7516 - val_loss: 1.0892\n",
      "Epoch 178/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.4155e-05 - val_accuracy: 0.7516 - val_loss: 1.0764\n",
      "Epoch 179/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1985e-04 - val_accuracy: 0.6930 - val_loss: 1.3978\n",
      "Epoch 180/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.7172e-06 - val_accuracy: 0.6939 - val_loss: 1.3951\n",
      "Epoch 181/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.0430e-06 - val_accuracy: 0.7045 - val_loss: 1.2431\n",
      "Epoch 182/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3349e-05 - val_accuracy: 0.7178 - val_loss: 1.1779\n",
      "Epoch 183/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.0491e-04 - val_accuracy: 0.6957 - val_loss: 1.2419\n",
      "Epoch 184/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6133e-05 - val_accuracy: 0.7427 - val_loss: 1.1819\n",
      "Epoch 185/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.4294e-04 - val_accuracy: 0.7507 - val_loss: 1.0819\n",
      "Epoch 186/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1823e-06 - val_accuracy: 0.7507 - val_loss: 1.0805\n",
      "Epoch 187/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.4649e-05 - val_accuracy: 0.7551 - val_loss: 1.0618\n",
      "Epoch 188/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.6841e-06 - val_accuracy: 0.7595 - val_loss: 0.9575\n",
      "Epoch 189/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.3413e-05 - val_accuracy: 0.7587 - val_loss: 0.9338\n",
      "Epoch 190/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.7380e-06 - val_accuracy: 0.7578 - val_loss: 0.9288\n",
      "Epoch 191/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.1241e-04 - val_accuracy: 0.6105 - val_loss: 1.4391\n",
      "Epoch 192/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.4793e-05 - val_accuracy: 0.6229 - val_loss: 1.3600\n",
      "Epoch 193/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9997 - loss: 5.6753e-04 - val_accuracy: 0.7498 - val_loss: 0.7968\n",
      "Epoch 194/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.9671e-05 - val_accuracy: 0.7445 - val_loss: 0.8062\n",
      "Epoch 195/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0930e-04 - val_accuracy: 0.7533 - val_loss: 0.8566\n",
      "Epoch 196/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.1433e-05 - val_accuracy: 0.7516 - val_loss: 1.0178\n",
      "Epoch 197/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9997 - loss: 0.0036 - val_accuracy: 0.7409 - val_loss: 1.4737\n",
      "Epoch 198/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.9888e-06 - val_accuracy: 0.7374 - val_loss: 1.3664\n",
      "Epoch 199/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.4526e-05 - val_accuracy: 0.7365 - val_loss: 1.2658\n",
      "Epoch 200/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.7595e-05 - val_accuracy: 0.7427 - val_loss: 1.1330\n",
      "Epoch 201/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5469e-04 - val_accuracy: 0.7196 - val_loss: 1.4643\n",
      "Epoch 202/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.9298e-05 - val_accuracy: 0.7196 - val_loss: 1.3522\n",
      "Epoch 203/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9998 - loss: 4.2464e-04 - val_accuracy: 0.7036 - val_loss: 0.9713\n",
      "Epoch 204/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.7200e-06 - val_accuracy: 0.7081 - val_loss: 0.9825\n",
      "Epoch 205/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.5424e-05 - val_accuracy: 0.7125 - val_loss: 0.9893\n",
      "Epoch 206/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5788e-06 - val_accuracy: 0.7143 - val_loss: 0.9862\n",
      "Epoch 207/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.6653e-05 - val_accuracy: 0.7169 - val_loss: 0.9871\n",
      "Epoch 208/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.3256e-05 - val_accuracy: 0.7178 - val_loss: 0.9904\n",
      "Epoch 209/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1970e-06 - val_accuracy: 0.7178 - val_loss: 0.9886\n",
      "Epoch 210/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.9810e-06 - val_accuracy: 0.7223 - val_loss: 0.9777\n",
      "Epoch 211/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.6407e-04 - val_accuracy: 0.7214 - val_loss: 0.9858\n",
      "Epoch 212/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.5553e-06 - val_accuracy: 0.7187 - val_loss: 1.0253\n",
      "Epoch 213/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.5985e-06 - val_accuracy: 0.7143 - val_loss: 1.0537\n",
      "Epoch 214/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.8234e-05 - val_accuracy: 0.6273 - val_loss: 1.4961\n",
      "Epoch 215/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.5866e-06 - val_accuracy: 0.6291 - val_loss: 1.4138\n",
      "Epoch 216/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.1714e-06 - val_accuracy: 0.6300 - val_loss: 1.3839\n",
      "Epoch 217/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.6038e-06 - val_accuracy: 0.6495 - val_loss: 1.2698\n",
      "Epoch 218/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.6599e-05 - val_accuracy: 0.6708 - val_loss: 1.2066\n",
      "Epoch 219/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.2950e-07 - val_accuracy: 0.6726 - val_loss: 1.1976\n",
      "Epoch 220/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.3730e-06 - val_accuracy: 0.6761 - val_loss: 1.1554\n",
      "Epoch 221/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0106e-04 - val_accuracy: 0.6752 - val_loss: 1.1645\n",
      "Epoch 222/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.2425e-04 - val_accuracy: 0.6735 - val_loss: 1.2111\n",
      "Epoch 223/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.2543e-05 - val_accuracy: 0.7063 - val_loss: 0.9676\n",
      "Epoch 224/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.2305e-06 - val_accuracy: 0.7232 - val_loss: 0.8259\n",
      "Epoch 225/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1508e-04 - val_accuracy: 0.7329 - val_loss: 0.9803\n",
      "Epoch 226/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2145e-06 - val_accuracy: 0.7214 - val_loss: 1.0984\n",
      "Epoch 227/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.9174e-06 - val_accuracy: 0.7187 - val_loss: 1.1524\n",
      "Epoch 228/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.1977e-04 - val_accuracy: 0.7187 - val_loss: 1.1702\n",
      "Epoch 229/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.9144e-05 - val_accuracy: 0.7205 - val_loss: 1.1571\n",
      "Epoch 230/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.3264e-06 - val_accuracy: 0.7214 - val_loss: 1.1136\n",
      "Epoch 231/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.3665e-05 - val_accuracy: 0.7214 - val_loss: 1.0696\n",
      "Epoch 232/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.2187e-06 - val_accuracy: 0.7276 - val_loss: 1.0405\n",
      "Epoch 233/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.8216e-06 - val_accuracy: 0.7320 - val_loss: 1.0234\n",
      "Epoch 234/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.8491e-06 - val_accuracy: 0.7294 - val_loss: 1.0298\n",
      "Epoch 235/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.3879e-05 - val_accuracy: 0.7276 - val_loss: 1.0710\n",
      "Epoch 236/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.8619e-05 - val_accuracy: 0.7276 - val_loss: 1.0693\n",
      "Epoch 237/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.5094e-07 - val_accuracy: 0.7285 - val_loss: 1.0801\n",
      "Epoch 238/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.8776e-05 - val_accuracy: 0.7223 - val_loss: 1.1101\n",
      "Epoch 239/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.0324e-06 - val_accuracy: 0.7240 - val_loss: 1.1048\n",
      "Epoch 240/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.2534e-07 - val_accuracy: 0.7232 - val_loss: 1.1076\n",
      "Epoch 241/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.7875e-06 - val_accuracy: 0.7223 - val_loss: 1.1162\n",
      "Epoch 242/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.3562e-07 - val_accuracy: 0.7303 - val_loss: 1.0718\n",
      "Epoch 243/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.5034e-05 - val_accuracy: 0.7480 - val_loss: 0.8594\n",
      "Epoch 244/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3592e-05 - val_accuracy: 0.7524 - val_loss: 0.8081\n",
      "Epoch 245/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.9127e-07 - val_accuracy: 0.7507 - val_loss: 0.8138\n",
      "Epoch 246/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3940e-05 - val_accuracy: 0.7453 - val_loss: 0.8320\n",
      "Epoch 247/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.0918e-07 - val_accuracy: 0.7436 - val_loss: 0.8739\n",
      "Epoch 248/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.4038e-04 - val_accuracy: 0.7445 - val_loss: 0.8948\n",
      "Epoch 249/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.5286e-04 - val_accuracy: 0.7471 - val_loss: 0.9118\n",
      "Epoch 250/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.0409e-05 - val_accuracy: 0.7551 - val_loss: 1.4561\n",
      "Epoch 251/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.3750e-05 - val_accuracy: 0.7427 - val_loss: 1.4366\n",
      "Epoch 252/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.3587e-07 - val_accuracy: 0.7436 - val_loss: 1.4039\n",
      "Epoch 253/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.4418e-05 - val_accuracy: 0.7471 - val_loss: 1.2878\n",
      "Epoch 254/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3789e-06 - val_accuracy: 0.7489 - val_loss: 1.0547\n",
      "Epoch 255/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0163e-06 - val_accuracy: 0.7569 - val_loss: 0.9993\n",
      "Epoch 256/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.2512e-07 - val_accuracy: 0.7489 - val_loss: 1.0487\n",
      "Epoch 257/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.3143e-05 - val_accuracy: 0.7516 - val_loss: 1.0240\n",
      "Epoch 258/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.6142e-06 - val_accuracy: 0.7161 - val_loss: 1.0320\n",
      "Epoch 259/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.2126e-07 - val_accuracy: 0.6832 - val_loss: 1.2704\n",
      "Epoch 260/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.7022e-05 - val_accuracy: 0.6965 - val_loss: 1.0096\n",
      "Epoch 261/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.9907e-06 - val_accuracy: 0.7453 - val_loss: 1.4141\n",
      "Epoch 262/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.9501e-07 - val_accuracy: 0.7400 - val_loss: 1.1668\n",
      "Epoch 263/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.7799e-05 - val_accuracy: 0.7320 - val_loss: 0.7289\n",
      "Epoch 264/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.1721e-05 - val_accuracy: 0.7374 - val_loss: 1.0880\n",
      "Epoch 265/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.0614e-05 - val_accuracy: 0.6273 - val_loss: 4.0749\n",
      "Epoch 266/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.7799e-05 - val_accuracy: 0.7311 - val_loss: 1.4181\n",
      "Epoch 267/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.2996e-07 - val_accuracy: 0.7365 - val_loss: 1.3297\n",
      "Epoch 268/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6575e-06 - val_accuracy: 0.7116 - val_loss: 1.5103\n",
      "Epoch 269/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.4314e-07 - val_accuracy: 0.7516 - val_loss: 1.0573\n",
      "Epoch 270/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.3681e-05 - val_accuracy: 0.7560 - val_loss: 1.0270\n",
      "Epoch 271/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 1.0966e-04 - val_accuracy: 0.7587 - val_loss: 2.8164\n",
      "Epoch 272/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9992 - loss: 0.0042 - val_accuracy: 0.7063 - val_loss: 3.1790\n",
      "Epoch 273/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9982 - loss: 0.0040 - val_accuracy: 0.7578 - val_loss: 1.3072\n",
      "Epoch 274/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.5421e-05 - val_accuracy: 0.7569 - val_loss: 1.2551\n",
      "Epoch 275/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 7.7701e-05 - val_accuracy: 0.7533 - val_loss: 0.9988\n",
      "Epoch 276/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.2854e-05 - val_accuracy: 0.7666 - val_loss: 1.0539\n",
      "Epoch 277/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.9359e-05 - val_accuracy: 0.7666 - val_loss: 1.0546\n",
      "Epoch 278/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.6954e-06 - val_accuracy: 0.7560 - val_loss: 1.0663\n",
      "Epoch 279/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.8847e-05 - val_accuracy: 0.7551 - val_loss: 1.0831\n",
      "Epoch 280/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.2866e-06 - val_accuracy: 0.7666 - val_loss: 1.0770\n",
      "Epoch 281/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9998 - loss: 0.0020 - val_accuracy: 0.6149 - val_loss: 2.6930\n",
      "Epoch 282/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.7276e-05 - val_accuracy: 0.7329 - val_loss: 1.3503\n",
      "Epoch 283/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.5479e-05 - val_accuracy: 0.7329 - val_loss: 1.3283\n",
      "Epoch 284/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5918e-05 - val_accuracy: 0.7542 - val_loss: 1.2169\n",
      "Epoch 285/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4209e-05 - val_accuracy: 0.7542 - val_loss: 1.2175\n",
      "Epoch 286/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.3229e-06 - val_accuracy: 0.7285 - val_loss: 1.1838\n",
      "Epoch 287/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.3572e-05 - val_accuracy: 0.7294 - val_loss: 1.2219\n",
      "Epoch 288/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.8546e-05 - val_accuracy: 0.7294 - val_loss: 1.2410\n",
      "Epoch 289/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.8710e-05 - val_accuracy: 0.7311 - val_loss: 1.2373\n",
      "Epoch 290/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.4334e-05 - val_accuracy: 0.7311 - val_loss: 1.2361\n",
      "Epoch 291/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.7965e-05 - val_accuracy: 0.7382 - val_loss: 1.2344\n",
      "Epoch 292/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1513e-05 - val_accuracy: 0.7382 - val_loss: 1.2348\n",
      "Epoch 293/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.1719e-05 - val_accuracy: 0.7356 - val_loss: 1.2257\n",
      "Epoch 294/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.8193e-07 - val_accuracy: 0.7374 - val_loss: 1.2260\n",
      "Epoch 295/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3296e-06 - val_accuracy: 0.7382 - val_loss: 1.2182\n",
      "Epoch 296/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.3287e-05 - val_accuracy: 0.7391 - val_loss: 1.2178\n",
      "Epoch 297/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.2339e-06 - val_accuracy: 0.7391 - val_loss: 1.2186\n",
      "Epoch 298/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1586e-06 - val_accuracy: 0.7418 - val_loss: 1.1951\n",
      "Epoch 299/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.8622e-05 - val_accuracy: 0.7445 - val_loss: 1.1929\n",
      "Epoch 300/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.9935e-06 - val_accuracy: 0.7418 - val_loss: 1.1888\n",
      "Epoch 301/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9995 - loss: 5.2806e-04 - val_accuracy: 0.6930 - val_loss: 1.2649\n",
      "Epoch 302/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.2138e-06 - val_accuracy: 0.6779 - val_loss: 1.3239\n",
      "Epoch 303/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.7817e-06 - val_accuracy: 0.6912 - val_loss: 1.2785\n",
      "Epoch 304/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.0193e-05 - val_accuracy: 0.7480 - val_loss: 1.4212\n",
      "Epoch 305/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.5299e-06 - val_accuracy: 0.7578 - val_loss: 1.1843\n",
      "Epoch 306/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.8035e-06 - val_accuracy: 0.7471 - val_loss: 1.1248\n",
      "Epoch 307/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.0819e-07 - val_accuracy: 0.7471 - val_loss: 1.1282\n",
      "Epoch 308/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.1841e-06 - val_accuracy: 0.7489 - val_loss: 1.1460\n",
      "Epoch 309/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.8479e-06 - val_accuracy: 0.7542 - val_loss: 1.1452\n",
      "Epoch 310/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9998 - loss: 9.6245e-04 - val_accuracy: 0.6806 - val_loss: 2.4160\n",
      "Epoch 311/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4789e-04 - val_accuracy: 0.6886 - val_loss: 2.2026\n",
      "Epoch 312/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.0577e-05 - val_accuracy: 0.6894 - val_loss: 2.1889\n",
      "Epoch 313/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.3517e-06 - val_accuracy: 0.6974 - val_loss: 2.0613\n",
      "Epoch 314/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3398e-05 - val_accuracy: 0.7498 - val_loss: 1.3665\n",
      "Epoch 315/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4855e-04 - val_accuracy: 0.7507 - val_loss: 1.3456\n",
      "Epoch 316/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.7678e-05 - val_accuracy: 0.7569 - val_loss: 1.3171\n",
      "Epoch 317/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3815e-05 - val_accuracy: 0.7560 - val_loss: 1.3091\n",
      "Epoch 318/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.0429e-06 - val_accuracy: 0.7524 - val_loss: 1.2917\n",
      "Epoch 319/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.6794e-06 - val_accuracy: 0.7587 - val_loss: 1.0804\n",
      "Epoch 320/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.3674e-06 - val_accuracy: 0.7578 - val_loss: 1.0822\n",
      "Epoch 321/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.3648e-06 - val_accuracy: 0.7622 - val_loss: 1.0869\n",
      "Epoch 322/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.5574e-07 - val_accuracy: 0.7604 - val_loss: 1.0845\n",
      "Epoch 323/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0281e-05 - val_accuracy: 0.7578 - val_loss: 1.1495\n",
      "Epoch 324/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.3894e-05 - val_accuracy: 0.7569 - val_loss: 1.1499\n",
      "Epoch 325/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.7502e-07 - val_accuracy: 0.7578 - val_loss: 1.1472\n",
      "Epoch 326/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.6765e-06 - val_accuracy: 0.7587 - val_loss: 1.1492\n",
      "Epoch 327/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4063e-06 - val_accuracy: 0.7578 - val_loss: 1.1504\n",
      "Epoch 328/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.1352e-07 - val_accuracy: 0.7587 - val_loss: 1.1434\n",
      "Epoch 329/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.8686e-07 - val_accuracy: 0.7587 - val_loss: 1.1475\n",
      "Epoch 330/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.8455e-07 - val_accuracy: 0.7595 - val_loss: 1.1480\n",
      "Epoch 331/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0864e-05 - val_accuracy: 0.7587 - val_loss: 1.1556\n",
      "Epoch 332/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.2386e-05 - val_accuracy: 0.7649 - val_loss: 1.1494\n",
      "Epoch 333/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.3194e-05 - val_accuracy: 0.7569 - val_loss: 1.3984\n",
      "Epoch 334/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.8877e-06 - val_accuracy: 0.7560 - val_loss: 1.3855\n",
      "Epoch 335/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.0740e-06 - val_accuracy: 0.7622 - val_loss: 1.3807\n",
      "Epoch 336/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.4118e-06 - val_accuracy: 0.7569 - val_loss: 1.4338\n",
      "Epoch 337/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.0647e-04 - val_accuracy: 0.7551 - val_loss: 1.4406\n",
      "Epoch 338/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.9694e-07 - val_accuracy: 0.7578 - val_loss: 1.4307\n",
      "Epoch 339/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.3084e-07 - val_accuracy: 0.7533 - val_loss: 1.4423\n",
      "Epoch 340/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.7191e-06 - val_accuracy: 0.7524 - val_loss: 1.7903\n",
      "Epoch 341/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.4890e-07 - val_accuracy: 0.7524 - val_loss: 1.7932\n",
      "Epoch 342/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1367e-04 - val_accuracy: 0.7516 - val_loss: 1.7912\n",
      "Epoch 343/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 1.3964e-04 - val_accuracy: 0.6282 - val_loss: 1.6930\n",
      "Epoch 344/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.7141e-05 - val_accuracy: 0.7418 - val_loss: 1.8294\n",
      "Epoch 345/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 2.8735e-04 - val_accuracy: 0.5661 - val_loss: 4.8620\n",
      "Epoch 346/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.6013e-05 - val_accuracy: 0.7542 - val_loss: 1.3796\n",
      "Epoch 347/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.4198e-06 - val_accuracy: 0.7374 - val_loss: 1.3773\n",
      "Epoch 348/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.1482e-06 - val_accuracy: 0.7533 - val_loss: 1.3524\n",
      "Epoch 349/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.0690e-06 - val_accuracy: 0.7657 - val_loss: 1.4610\n",
      "Epoch 350/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 7.1937e-04 - val_accuracy: 0.7453 - val_loss: 1.8427\n",
      "Epoch 351/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 0.0029 - val_accuracy: 0.7613 - val_loss: 1.4981\n",
      "Epoch 352/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.7299e-06 - val_accuracy: 0.7622 - val_loss: 1.5046\n",
      "Epoch 353/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.5341e-05 - val_accuracy: 0.7622 - val_loss: 1.5121\n",
      "Epoch 354/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.6515e-05 - val_accuracy: 0.7613 - val_loss: 1.5153\n",
      "Epoch 355/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.0237e-05 - val_accuracy: 0.7542 - val_loss: 1.3892\n",
      "Epoch 356/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.1419e-04 - val_accuracy: 0.7578 - val_loss: 1.3721\n",
      "Epoch 357/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.4770e-06 - val_accuracy: 0.7595 - val_loss: 1.3728\n",
      "Epoch 358/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.2606e-06 - val_accuracy: 0.7569 - val_loss: 1.3312\n",
      "Epoch 359/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.4501e-06 - val_accuracy: 0.7587 - val_loss: 1.3373\n",
      "Epoch 360/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.4235e-04 - val_accuracy: 0.7587 - val_loss: 1.3557\n",
      "Epoch 361/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.9677e-07 - val_accuracy: 0.7560 - val_loss: 1.3658\n",
      "Epoch 362/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.0719e-06 - val_accuracy: 0.7578 - val_loss: 1.3633\n",
      "Epoch 363/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.1246e-07 - val_accuracy: 0.7587 - val_loss: 1.3664\n",
      "Epoch 364/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.8561e-07 - val_accuracy: 0.7587 - val_loss: 1.3681\n",
      "Epoch 365/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.8149e-07 - val_accuracy: 0.7587 - val_loss: 1.3693\n",
      "Epoch 366/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.3253e-06 - val_accuracy: 0.7578 - val_loss: 1.3752\n",
      "Epoch 367/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.3941e-05 - val_accuracy: 0.7533 - val_loss: 1.3890\n",
      "Epoch 368/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.4037e-05 - val_accuracy: 0.7524 - val_loss: 1.3938\n",
      "Epoch 369/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0440e-05 - val_accuracy: 0.7524 - val_loss: 1.3939\n",
      "Epoch 370/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.9554e-07 - val_accuracy: 0.7524 - val_loss: 1.3937\n",
      "Epoch 371/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.0584e-06 - val_accuracy: 0.7533 - val_loss: 1.3964\n",
      "Epoch 372/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.4392e-05 - val_accuracy: 0.7533 - val_loss: 1.3953\n",
      "Epoch 373/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.7095e-05 - val_accuracy: 0.7533 - val_loss: 1.3985\n",
      "Epoch 374/500\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.3023e-07 - val_accuracy: 0.7533 - val_loss: 1.4038\n",
      "Epoch 375/500\n",
      "\u001b[1m 74/141\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.2756e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('D02fully_processed_telco_data.csv')\n",
    "\n",
    "# Check for any non-numeric columns that need to be encoded or removed\n",
    "print(df.dtypes)  # This will help in identifying non-numeric columns\n",
    "\n",
    "# Assuming 'Churn_Yes' is the target\n",
    "X = df.drop('Churn_Yes', axis=1)\n",
    "y = df['Churn_Yes'].values\n",
    "\n",
    "# Automatically identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define a pipeline to transform data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Applying transformations\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural Network Model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87dd3fa-66be-4901-a4de-e04b62336e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
